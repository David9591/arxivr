# SC-FEGAN: Face Editing Generative Adversarial Network with User's Sketch and Color
(arxiv:1902.06838)

```
@InProceedings{Jo_2019_ICCV,
  author = {Jo, Youngjoo and Park, Jongyoul},
  title = {SC-FEGAN: Face Editing Generative Adversarial Network With User's Sketch and Color},
  booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
  month = {October},
  year = {2019}
}
```

Read The Complete paper <a href='https://arxiv.org/abs/1902.06838'>here</a><br />
Evaluation Code From Authors Available <a href='https://github.com/run-youngjoo/SC-FEGAN'>here</a>

<br />

_____________________________________________________________________________________
#### Summary

This paper introduces a novel method for editing or generating facial features using guiding information from line strokes and color information.
Certain parts of the face can be masked and reconstructed providing guiding information in the form of line strokes. Furthermore, the masks can be painted with certain colors to provide the color information which is used by the model to regenerate the painted region with that color information.
The techniques mentioned could be useful for regenerating obstructed or damaged facial images, or to introduce new features to a face. A new face can also be generated by masking the whole face.
The model depends on the generator generating real like features, and trains using custom GAN Loss along with specific loss functions for the discriminator and generator.

_____________________________________________________________________________________

#### Architecture

1. *<b>Discriminator</b>*
>* Discriminator has <a href='https://arxiv.org/abs/1806.03589'>SN-PatchGAN</a> structure.
>* ReLU isn't used and 3x3 kernels are used.
>* Additional gradient penalty term is used to avoid output patch reaching values close to zero.
<br/>

2. *<b>Generator</b>*
>* Generator is based on U-Net like autoencoder architecture.
>* The core architecture is that of U-Net with the convolutions replaced with <a href='https://arxiv.org/abs/1806.03589'>gated convolutions</a>.
>* The kernels are all 3x3
>* <a href='https://arxiv.org/abs/1710.10196'>Local signal normalization (LRN)</a> is used for all convolutions except input and output layers after the feature map convolutions.
>* Generator recieves HxWx9 input, where 9 channels are.
>>* 3x RGB input image.
>>* 3x for the color information for the masked region.
>>* 1x for binary mask (masked region).
>>* 1x for the stroke information (binary).
>>* 1x for binary noise.
>* The encoder of the generator downsamples input 7 times using stride = 2.
>* Dilated convolutions are used before upsampling.
>* Decoder uses transposed convolutions.
>* ReLU activation is used for all hidden layers.
>* Output Layer is tanh activated.
>* There are 16 hidden layers in total in the generator.
>* The output is an RGB image of the same dimesions as the input.
>* Only the masked part of the image from the generator is used and the rest of the image is the original image for maintaining quality of the image and avoid additional noise in reconstruction and this allows for the generator to be trained on the masked region exclusively.

<br/>


<img src='./arch.png' alt='spatio-temporal video autoencoder'/>

_____________________________________________________________________________________
#### Training Process

_____________________________________________________________________________________
##### Loss Functions

* I<sub>comp</sub> = Completed image. (The original image with the masked region replaced by the generators activation for the masked region)

* I<sub>gen</sub> = Output Of The Generator.

* I<sub>gt</sub> = The input image.

* N<sub>a</sub> = Number of elements of <b><i>a</i></b>

* M = mask

* $\theta_{q}$ = Feature map of the qth layer of the VGG-16

* $\beta$ = 0.001

* $\sigma$ = 0.05

* $\gamma$ = 120

* $\epsilon$ = 0.001

* $\theta$ = 10

* $\nu$ = 0.1

1. <b>L<sub>G_SN</sub></b> = $-D(I_{comp})$ => -1 * The activation of discriminator on the completed image.

2. <b>L<sub>ppxl</sub></b> = $\frac{1}{N_{gt}}\cdot|| M \odot (I_{gen} - I_{gt})|| + \frac{\alpha}{N_{gt}}||(1-M)\odot(I_{gen}-I_{gt})||$ => Calculates $L_{1}$ distance between the ground truth and the generated image.

3. <b>L<sub>percept</sub></b> = $\sum_{q}||\frac{\theta_{q}(I_{gen}) - \theta_{q}(I_{gt})}{N_{\theta_{q}(I_{gt})}}||+\sum_{q}||\frac{\theta_{q}(I_{comp}) - \theta_{q}(I_{gt})}{N_{\theta_{q}(I_{gt})}}||$ => Computes the $L_{1}$ loss in the feature space. A Pretrained VGG-16 model trained on face recognition is used to generate face encodings for both the completed(original image with masked part replaced by the generated) and whole generated image and the $L_{1}$ distance is calculated with the original image encodings.
Only a few layer activations are used instead of the final activations. In the paper pool1, pool2 and pool3 layers are used.

4. <b>L<sub>style</sub>(I)</b> = $\sum_{q}\frac{1}{C_{q}C_{q}}\cdot||\frac{(G_{q}(I) - G_{q}(I_{gt}))}{N_{q}}||$ , where $G_{q} = (\theta_{q}(x)^T(\theta-q(x)))$ is the Gram matrix to perform autocorrelation on each feature map of VGG-16 and $q(x)$ is the activation of the qth layer. => Computes the content of the two images using gram matrix.

5. <b>L<sub>tv</sub></b> =

> <b>L<sub>tv-row</sub></b><br/>
> <b>L<sub>tv-col</sub></b>

6. <b>L<sub>G</sub></b> = $\mathbb{E}[(||\nabla_{U}D(U)\odot M||_{2} -1)^2]$ => The WGAN Loss.
_____________________________________________________________________________________